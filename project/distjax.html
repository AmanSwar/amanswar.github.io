
<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Project - DistJax</title>
    <style>
        :root {
            --bg-primary: #0a0a0b;
            --bg-secondary: #141417;
            --bg-card: #1a1a1f;
            --text-primary: #ffffff;
            --text-secondary: #a0a0aa;
            --text-muted: #6b6b7a;
            --accent-primary: #00d4ff;
            --accent-secondary: #7c3aed;
            --accent-gradient: linear-gradient(135deg, #00d4ff 0%, #7c3aed 100%);
            --border-subtle: rgba(255, 255, 255, 0.1);
            --shadow-glow: rgba(0, 212, 255, 0.2);
            --warning: #ff6b35;
        }

        [data-theme="light"] {
            --bg-primary: #ffffff;
            --bg-secondary: #f8f9fa;
            --bg-card: #ffffff;
            --text-primary: #1a1a1b;
            --text-secondary: #4a4a5a;
            --text-muted: #8a8a9a;
            --accent-primary: #0066cc;
            --accent-secondary: #7c3aed;
            --accent-gradient: linear-gradient(135deg, #0066cc 0%, #7c3aed 100%);
            --border-subtle: rgba(0, 0, 0, 0.1);
            --shadow-glow: rgba(0, 102, 204, 0.1);
            --warning: #ff6b35;
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', system-ui, sans-serif;
            background: var(--bg-primary);
            color: var(--text-primary);
            line-height: 1.6;
            transition: all 0.3s ease;
        }
        .container {
            max-width: 800px;
            margin: 0 auto;
            padding: 2rem;
        }

        h1, h2, h3, h4, h5, h6 {
            color: var(--text-primary);
            margin-bottom: 1rem;
        }

        p {
            color: var(--text-secondary);
            margin-bottom: 1rem;
        }

        a {
            color: var(--accent-primary);
            text-decoration: none;
        }

        a:hover {
            text-decoration: underline;
        }

        code {
            background: var(--bg-secondary);
            padding: 0.2rem 0.4rem;
            border-radius: 4px;
            font-family: 'Monaco', monospace;
        }

        pre {
            background: var(--bg-secondary);
            padding: 1rem;
            border-radius: 8px;
            overflow-x: auto;
        }

        pre code {
            padding: 0;
            background: none;
        }
    </style>
</head>

<body>
    <div class="container">
        <h1>DistJax</h1>
        <h3>1. Introduction</h3>
        <p>DistJax is a powerful and flexible library for JAX that simplifies the implementation of distributed training for large-scale neural networks. It provides high-level abstractions and reusable components for common parallelism strategies, enabling you to scale your models with minimal code changes.</p>
        <p><strong>Key Features:</strong></p>
        <ul>
            <li><strong>Data Parallelism:</strong> Train your model on multiple devices with data sharding.</li>
            <li><strong>Tensor Parallelism:</strong> Shard your model's weights across multiple devices to train models that don't fit on a single device.</li>
            <li><strong>Pipeline Parallelism:</strong> Partition your model's layers across multiple devices to train very deep models efficiently.</li>
            <li><strong>Asynchronous Communication:</strong> Overlap communication and computation to maximize hardware utilization.</li>
            <li><strong>Composable Primitives:</strong> Mix and match different parallelism strategies to create hybrid solutions.</li>
        </ul>
        <h3>2. Core Concepts</h3>
        <h4>a. Device Mesh</h4>
        <p>A core concept in DistJax is the <strong>device mesh</strong>, which is a logical grid of devices (e.g., GPUs or TPUs) that you can use to distribute your model and data. The mesh is defined by a shape and a set of axis names, which you can use to specify how your data and model are sharded.</p>
        <pre><code>from jax.experimental.maps import Mesh
import numpy as np

num_devices = 8
# Create a 1D mesh for data parallelism
dp_mesh = Mesh(np.arange(num_devices).reshape(num_devices,), ('data',))

# Create a 2D mesh for data and tensor parallelism
tp_mesh = Mesh(np.arange(num_devices).reshape(4, 2), ('data', 'tensor'))
</code></pre>
        <h4>b. Sharding</h4>
        <p>DistJax uses <code>shard_map</code> and <code>PartitionSpec</code> to control how your data and model are sharded across the device mesh. A <code>PartitionSpec</code> is a tuple that specifies how each dimension of a tensor is sharded across the mesh axes.</p>
        <pre><code>from jax.experimental.maps import PartitionSpec as P

# Replicate the weights across the 'data' axis
replicated = P()

# Shard the weights along the 'tensor' axis
sharded = P('tensor',)
</code></pre>
        <h3>3. Parallelism Strategies</h3>
        <h4>a. Data Parallelism</h4>
        <p>Data parallelism is the most common and straightforward way to distribute your training. In this strategy, you replicate your model on each device and feed each device a different shard of the input data. Gradients are then averaged across all devices to update the model's weights.</p>
        <p><strong>Example:</strong></p>
        <pre><code>from DistJax.parallelism.data_parallel import data_parallel_train_step
from DistJax.models.simple_classifier import SimpleClassifier

# Define your model
model = SimpleClassifier(num_classes=10)

# Create a data-parallel training step
train_step = data_parallel_train_step(model)

# Run the training step on sharded data
sharded_train_step = shard_map(
    train_step,
    mesh=dp_mesh,
    in_specs=(P('data',), P('data',)),
    out_specs=P()
)
</code></pre>
        <h4>b. Tensor Parallelism</h4>
        <p>Tensor parallelism is a model parallelism technique where you shard the model's weights across multiple devices. This allows you to train models that are too large to fit on a single device. DistJax provides both synchronous and asynchronous tensor parallelism.</p>
        <p><strong>Synchronous Tensor Parallelism:</strong></p>
        <p>In synchronous tensor parallelism, communication is performed using collective operations like <code>all_gather</code> and <code>psum_scatter</code>.</p>
        <p><strong>Asynchronous Tensor Parallelism:</strong></p>
        <p>Asynchronous tensor parallelism overlaps communication and computation to hide communication latency. This is achieved using JAX's <code>ppermute</code> operation to pass activations between devices in a ring-like fashion.</p>
        <p><strong>Example:</strong></p>
        <pre><code>from DistJax.parallelism.tensor_parallel import TPDense
from DistJax.parallelism.tensor_parallel_async import TPAsyncDense

# Synchronous tensor-parallel dense layer
dense_layer = TPDense(
    features=1024,
    kernel_init=jax.nn.initializers.glorot_normal(),
    mesh=tp_mesh,
)

# Asynchronous tensor-parallel dense layer
async_dense_layer = TPAsyncDense(
    features=1024,
    kernel_init=jax.nn.initializers.glorot_normal(),
    mesh=tp_mesh,
)
</code></pre>
        <h4>c. Pipeline Parallelism</h4>
        <p>Pipeline parallelism is another model parallelism technique where you partition the layers of your model across multiple devices. The input batch is split into micro-batches, which are fed into the pipeline in a staggered manner to keep all devices active.</p>
        <p><strong>Example:</strong></p>
        <pre><code>from DistJax.parallelism.pipeline_parallel import pipeline_parallel_train_step
from DistJax.models.pp_classifier import PPClassifier

# Define your model
model = PPClassifier(num_classes=10, num_layers=4)

# Create a pipeline-parallel training step
train_step = pipeline_parallel_train_step(model, num_micro_batches=4)

# Run the training step
sharded_train_step = shard_map(
    train_step,
    mesh=pp_mesh,
    in_specs=(P('data',), P('data',)),
    out_specs=P()
)
</code></pre>
        <h3>4. Models</h3>
        <p>DistJax provides several example models that demonstrate how to use the different parallelism strategies:</p>
        <ul>
            <li><strong><code>SimpleClassifier</code>:</strong> A basic classifier for demonstrating data parallelism.</li>
            <li><strong><code>TPClassifier</code>:</strong> A classifier that uses tensor parallelism.</li>
            <li><strong><code>PPClassifier</code>:</strong> A classifier that uses pipeline parallelism.</li>
            <li><strong><code>Transformer</code>:</strong> A Transformer model with tensor parallelism.</li>
        </ul>
        <h3>5. Getting Started</h3>
        <p>To get started with DistJax, please refer to the <code>README.md</code> file for installation instructions and examples.</p>
        <h3>6. API Reference</h3>
        <p>The core components of DistJax are located in the <code>DistJax/parallelism</code> directory:</p>
        <ul>
            <li><strong><code>data_parallel.py</code>:</strong> Data parallelism primitives.</li>
            <li><strong><code>tensor_parallel.py</code>:</strong> Synchronous tensor parallelism primitives.</li>
            <li><strong><code>tensor_parallel_async.py</code>:</strong> Asynchronous tensor parallelism primitives.</li>
            <li><strong><code>pipeline_parallel.py</code>:</strong> Pipeline parallelism primitives.</li>
            <li><strong><code>sharding.py</code>:</strong> Utilities for sharding.</li>
        </ul>
        <h3>7. Contributing</h3>
        <p>Contributions are welcome! Please feel free to open an issue or submit a pull request on our GitHub repository.</p>
        <a href="../projects.html" class="btn btn-secondary">Go Back</a>
    </div>
</body>

</html>
