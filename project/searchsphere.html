
<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Project - Search Sphere</title>
    <style>
        :root {
            --bg-primary: #0a0a0b;
            --bg-secondary: #141417;
            --bg-card: #1a1a1f;
            --text-primary: #ffffff;
            --text-secondary: #a0a0aa;
            --text-muted: #6b6b7a;
            --accent-primary: #00d4ff;
            --accent-secondary: #7c3aed;
            --accent-gradient: linear-gradient(135deg, #00d4ff 0%, #7c3aed 100%);
            --border-subtle: rgba(255, 255, 255, 0.1);
            --shadow-glow: rgba(0, 212, 255, 0.2);
            --warning: #ff6b35;
        }

        [data-theme="light"] {
            --bg-primary: #ffffff;
            --bg-secondary: #f8f9fa;
            --bg-card: #ffffff;
            --text-primary: #1a1a1b;
            --text-secondary: #4a4a5a;
            --text-muted: #8a8a9a;
            --accent-primary: #0066cc;
            --accent-secondary: #7c3aed;
            --accent-gradient: linear-gradient(135deg, #0066cc 0%, #7c3aed 100%);
            --border-subtle: rgba(0, 0, 0, 0.1);
            --shadow-glow: rgba(0, 102, 204, 0.1);
            --warning: #ff6b35;
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', system-ui, sans-serif;
            background: var(--bg-primary);
            color: var(--text-primary);
            line-height: 1.6;
            transition: all 0.3s ease;
        }
        .container {
            max-width: 800px;
            margin: 0 auto;
            padding: 2rem;
        }

        h1, h2, h3, h4, h5, h6 {
            color: var(--text-primary);
            margin-bottom: 1rem;
        }

        p {
            color: var(--text-secondary);
            margin-bottom: 1rem;
        }

        a {
            color: var(--accent-primary);
            text-decoration: none;
        }

        a:hover {
            text-decoration: underline;
        }

        code {
            background: var(--bg-secondary);
            padding: 0.2rem 0.4rem;
            border-radius: 4px;
            font-family: 'Monaco', monospace;
        }

        pre {
            background: var(--bg-secondary);
            padding: 1rem;
            border-radius: 8px;
            overflow-x: auto;
        }

        pre code {
            padding: 0;
            background: none;
        }
    </style>
</head>

<body>
    <div class="container">
        <h1>Search Sphere - Technical Documentation</h1>
        <h3>1. Introduction</h3>
        <p>Search Sphere is a standalone, AI-powered semantic search engine that runs locally on a user's machine. It is designed to overcome the limitations of traditional keyword-based file search by understanding the <em>meaning</em> behind a user's query. By leveraging state-of-the-art machine learning models, it can find relevant documents and images even if the query's keywords are not present in the file's name or content.</p>
        <p>This document provides a detailed technical overview of the system's architecture, pipelines, and core components.</p>
        <h3>2. System Architecture</h3>
        <p>The application is divided into two main packages: <code>encoder</code> and <code>query</code>, which work together to provide the core functionality. The <code>run.py</code> script serves as the main entry point, orchestrating the overall workflow and managing the user interface.</p>
        <ul>
            <li><strong><code>run.py</code> (Main Application Runner):</strong>
                <ul>
                    <li>Handles all user interaction through a CLI built with the <code>Rich</code> library.</li>
                    <li>Initializes the application, including a startup sequence and welcome message.</li>
                    <li>Prompts the user for a directory to index.</li>
                    <li>Calls the <code>encoder</code> module to perform the indexing.</li>
                    <li>Calls the <code>query</code> module to handle the interactive search loop.</li>
                </ul>
            </li>
            <li><strong><code>encoder</code> Package (The Indexer):</strong>
                <ul>
                    <li><strong><code>main_seq.py</code>:</strong> Contains the primary logic for the indexing pipeline. It traverses the file system, orchestrates content extraction and embedding generation, and saves the final index.</li>
                    <li><strong><code>embedding.py</code>:</strong> A critical module that uses the <strong>MobileCLIP</strong> model to convert text strings and image files into 512-dimensional vector embeddings.</li>
                    <li><strong><code>faiss_base.py</code>:</strong> A wrapper class, <code>FAISSManagerHNSW</code>, that abstracts away the complexity of managing the <strong>FAISS</strong> vector indexes. It handles adding new vectors, training the index (if necessary), and saving/loading the index from disk.</li>
                    <li><strong><code>utils.py</code>:</strong> A set of helper functions for tasks like extracting text from various file formats (<code>.pdf</code>, <code>.docx</code>, etc.) and retrieving file metadata.</li>
                </ul>
            </li>
            <li><strong><code>query</code> Package (The Searcher):</strong>
                <ul>
                    <li><strong><code>query.py</code>:</strong> The heart of the search functionality. It takes a user's raw query, uses the <code>utils</code> module to determine the query's intent, generates a query embedding, and performs the search against the FAISS index.</li>
                    <li><strong><code>utils.py</code>:</strong> Contains the <code>index_token</code> function, which loads the fine-tuned <strong>MobileBERT</strong> model to classify the user's query as either <code>TEXT</code> or <code>IMAGE</code>.</li>
                    <li><strong><code>fine_tune.py</code>:</strong> A utility script (not part of the main application flow) used to train the MobileBERT model for the intent classification task.</li>
                </ul>
            </li>
        </ul>
        <h3>3. The Indexing Pipeline</h3>
        <p>The indexing process is a sequential flow that builds the knowledge base for the search engine. This is handled by <code>encoder/main_seq.py</code>.</p>
        <ol>
            <li><strong>Initialization:</strong> The <code>FAISSManagerHNSW</code> is initialized, and any existing FAISS indexes (<code>index/text_index.index</code>, <code>index/image_index.index</code>) and metadata files are loaded into memory.</li>
            <li><strong>File Traversal:</strong> The application walks through the user-specified directory (<code>os.walk</code>) and compiles a list of all files that match the supported extensions (defined in <code>encoder/config.py</code>).</li>
            <li><strong>Content Processing Loop:</strong> Each file is processed one by one:
                <ol>
                    <li><strong>Content Extraction:</strong> Based on the file extension, the appropriate function from <code>encoder/utils.py</code> is called to extract the content. For text files, this is the raw text; for images, it is the file path.</li>
                    <li><strong>Embedding Generation:</strong> The extracted content (or image path) is passed to the <code>embedding.py</code> module. The <code>MobileCLIP</code> model then generates a 512-dimension floating-point vector.</li>
                    <li><strong>Temporary Storage:</strong> The generated embedding and its associated metadata (file path, name) are temporarily stored in a list within the <code>FAISSManagerHNSW</code> instance.</li>
                </ol>
            </li>
            <li><strong>Batch Indexing:</strong> Once all files have been processed, the <code>train_add</code> method of the <code>FAISSManagerHNSW</code> is called. This method stacks all the temporarily stored embeddings into a single NumPy array and adds them to the FAISS index. The <code>HNSWFlat</code> index type is highly efficient for this, as it does not require an explicit training step like other FAISS indexes (e.g., IVF).</li>
            <li><strong>Save to Disk:</strong> Finally, the <code>save_state</code> method is called to write the updated FAISS indexes and metadata JSON files to the <code>index/</code> directory, ensuring persistence between sessions.</li>
        </ol>
        <h3>4. The Search Pipeline</h3>
        <p>The search process is designed to be fast and accurate, providing relevant results in real-time. This is handled by <code>query/query.py</code>.</p>
        <ol>
            <li><strong>User Input:</strong> The user enters a natural language query into the CLI.</li>
            <li><strong>Query Intent Classification:</strong>
                <ol>
                    <li>The query string is passed to the <code>index_token</code> function in <code>query/utils.py</code>.</li>
                    <li>This function uses a fine-tuned <code>MobileBERT</code> model to perform a sequence classification task. The model outputs a label, either <code>TEXT</code> or <code>IMAGE</code>, based on what it believes the user is looking for.</li>
                </ol>
            </li>
            <li><strong>Query Embedding:</strong>
                <ol>
                    <li>The same query string is passed to the <code>text_extract</code> function in <code>encoder/embedding.py</code>.</li>
                    <li>The <code>MobileCLIP</code> model converts the query into a 512-dimensional vector embedding. This is the same model used for indexing, which is crucial for ensuring that the query and the indexed content exist in the same "embedding space."</li>
                </ol>
            </li>
            <li><strong>Similarity Search:</strong>
                <ol>
                    <li>Based on the intent (<code>TEXT</code> or <code>IMAGE</code>), the appropriate search method (<code>search_text</code> or <code>search_image</code>) in <code>FAISSManagerHNSW</code> is called.</li>
                    <li>FAISS then performs a k-Nearest Neighbor (k-NN) search on the corresponding vector index. It calculates the distance between the query vector and all the vectors in the index and returns the <code>k</code> vectors with the smallest distance (i.e., the highest similarity).</li>
                </ol>
            </li>
            <li><strong>Results Presentation:</strong>
                <ol>
                    <li>The search returns the IDs and similarity scores of the top matching items.</li>
                    <li>The application looks up the metadata (file name and path) for these IDs from the loaded metadata map.</li>
                    <li>The final results are displayed to the user in a formatted table using the <code>Rich</code> library.</li>
                </ol>
            </li>
        </ol>
        <h3>5. Key Technologies and Rationale</h3>
        <ul>
            <li><strong>MobileCLIP:</strong> Chosen for its excellent balance of performance and efficiency. It is designed for mobile and edge devices, making it lightweight enough to run quickly on a local machine while still providing high-quality, cross-modal embeddings for both text and images.</li>
            <li><strong>FAISS (HNSWFlat):</strong> The <code>HNSW (Hierarchical Navigable Small World)</code> graph-based index was chosen for its exceptional search speed and accuracy, especially for large datasets. Unlike <code>IVF</code> indexes, <code>HNSW</code> does not require a separate training phase, which makes it ideal for this application where new files can be added dynamically.</li>
            <li><strong>MobileBERT:</strong> A compact and fast variant of the BERT model. It was chosen for the query classification task because it is lightweight and provides excellent performance on classification tasks without introducing significant latency into the search pipeline.</li>
            <li><strong>Rich:</strong> This library was chosen to create a modern, visually appealing, and user-friendly CLI. It provides out-of-the-box components for progress bars, formatted tables, and styled text, which greatly enhances the overall user experience compared to a standard print-based interface.</li>
        </ul>
        <a href="../projects.html" class="btn btn-secondary">Go Back</a>
    </div>
</body>

</html>
